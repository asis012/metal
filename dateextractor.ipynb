{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dateextractor.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asis012/metal/blob/master/dateextractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1yEy4sp1WYv",
        "colab_type": "code",
        "outputId": "052c3d6b-82af-4b8b-8eea-264b7b354850",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skRUpAXY14de",
        "colab_type": "code",
        "outputId": "fcd72ebc-6eed-492d-da16-ca8aad6992f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkrJ4Nan1W_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cd = \"/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master\"\n",
        "path = \"/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc/parsed/doc_date.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2oApFVW17ui",
        "colab_type": "code",
        "outputId": "629cd28e-979e-4c6a-e0b9-be54cd853277",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc/parsed/doc_date.txt\", header = None)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>text</td>\n",
              "      <td>entity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>text,entity ABCX/Infosys Limited  Statement of...</td>\n",
              "      <td>(ORGANIZATION Work/NNP Infosys/NNP Limited/NNP)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>text,entity ABCX/Infosys Limited  Statement of...</td>\n",
              "      <td>(PERSON Arjun/NNP Das/NNP)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>text,entity ABCX/Infosys Limited  Statement of...</td>\n",
              "      <td>(ORGANIZATION ORGANIZATION/NNP)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>text,entity ABCX/Infosys Limited  Statement of...</td>\n",
              "      <td>(ORGANIZATION Work/NNP Infosys/NNP Limited/NNP)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                1\n",
              "0                                               text                                           entity\n",
              "1  text,entity ABCX/Infosys Limited  Statement of...  (ORGANIZATION Work/NNP Infosys/NNP Limited/NNP)\n",
              "2  text,entity ABCX/Infosys Limited  Statement of...                       (PERSON Arjun/NNP Das/NNP)\n",
              "3  text,entity ABCX/Infosys Limited  Statement of...                  (ORGANIZATION ORGANIZATION/NNP)\n",
              "4  text,entity ABCX/Infosys Limited  Statement of...  (ORGANIZATION Work/NNP Infosys/NNP Limited/NNP)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJqGHBti2HA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "simport os\n",
        "\n",
        "# Make sure we're running from the spam/ directory\n",
        "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
        "    os.chdir(\"getting_started\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYiXCTH3LXwQ",
        "colab_type": "code",
        "outputId": "ad47d247-c338-41cf-b275-40aaab1e355c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "cd /content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-344de15782be>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    cd /content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRZtLXXjLxYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import glob\n",
        "# import pandas as pd\n",
        "# import nltk\n",
        "# from nltk.tokenize import PunktSentenceTokenizer\n",
        "# from nltk.corpus import webtext\n",
        "# from dateparser.search import search_dates\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from nltk.tag import pos_tag\n",
        "\n",
        "# orgs = []\n",
        "\n",
        "# def traversetree(t):\n",
        "#     try:\n",
        "#         if (t.label()=='ORGANIZATION' or t.label()=='PERSON' or t.label()=='GPE'):\n",
        "#             orgs.append(str(t))\n",
        "#         else:\n",
        "#             for child in t:\n",
        "#                 traversetree(child)\n",
        "#     except AttributeError:\n",
        "#         return\n",
        "\n",
        "# def extractentity(x):\n",
        "#     ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(x)))\n",
        "#     global orgs\n",
        "#     orgs = []\n",
        "#     traversetree(ne_tree)\n",
        "#     return orgs\n",
        "\n",
        "# def extractdates(x):\n",
        "#     dates = search_dates(x.lower())\n",
        "#     if dates is not None:\n",
        "#         return dates\n",
        "#     else:\n",
        "#         return list()\n",
        "\n",
        "\n",
        "# def load_unlabeled_sentences_dataset(task):\n",
        "#     \"\"\"Load doc and extract sentences.\"\"\"\n",
        "#     filenames = sorted(glob.glob(\"*.txt\"))\n",
        "#     doc_dfs = []\n",
        "#     for i, filename in enumerate(filenames, start=1):\n",
        "#         df = pd.DataFrame()\n",
        "#         text = webtext.raw(filename)\n",
        "#         sent_tokenizer = PunktSentenceTokenizer(text)\n",
        "#         df[\"text\"] = sent_tokenizer.tokenize(text)\n",
        "#         df[\"text\"] = df[\"text\"].apply(lambda x: ''.join([\" \" if ord(i) < 32 or ord(i) > 126 else i for i in x]))\n",
        "\n",
        "#         #extract dates\n",
        "#         if task == \"date\":\n",
        "#             df[\"date\"] = df[\"text\"].apply(extractdates)\n",
        "#             doc_date = pd.DataFrame(df.date.tolist(), index=df.text).stack()\n",
        "#             doc_date = doc_date.reset_index([0, 'text'])\n",
        "#             doc_date = doc_date.rename(columns={0: 'date'})\n",
        "#             doc_dfs.append(doc_date)\n",
        "\n",
        "#         #extract entity\n",
        "#         if task == \"entity\":\n",
        "#             df[\"entity\"] = df[\"text\"].apply(extractentity)\n",
        "#             doc_entity = pd.DataFrame(df.entity.tolist(), index=df.text).stack()\n",
        "#             doc_entity = doc_entity.reset_index([0, 'text'])\n",
        "#             doc_entity = doc_entity.rename(columns={0: 'entity'})\n",
        "#             doc_dfs.append(doc_entity)\n",
        "\n",
        "#     if task == \"date\":\n",
        "#         pd.concat(doc_dfs).to_csv(path_or_buf='/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc/parsed/doc_date.txt', index=False)\n",
        "#     if task == \"entity\":\n",
        "#         pd.concat(doc_dfs).to_csv(path_or_buf='/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc/parsed/doc_entity.txt', index=False)\n",
        "\n",
        "\n",
        "# load_unlabeled_sentences_dataset(\"entity\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8keeVk18pPDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"entity\"] = df[\"text\"].apply(extractentity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSq9-Zu8pxGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_entity = pd.DataFrame(df.entity.tolist(), index=df.text).stack()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyNaNuBXp-rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"entity\"] = df[\"text\"].apply(extractentity)\n",
        "doc_entity = pd.DataFrame(df.entity.tolist(), index=df.text).stack()\n",
        "doc_entity = doc_entity.reset_index([0, 'text'])\n",
        "doc_entity = doc_entity.rename(columns={0: 'entity'})\n",
        "doc_dfs.append(doc_entity)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZcqXpnBrvuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.concat(doc_dfs).to_csv(path_or_buf='/content/drive/My Drive/snorkel-tutorial-master/snorkel-tutorial-master/doc/parsed/doc_date.txt', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNS1zVSfubSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmLWo2l2Ii9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_text = state_union.raw(path)\n",
        "sample_text = state_union.raw(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0zuLbqWI3GK",
        "colab_type": "code",
        "outputId": "9b43a13e-37d3-46f8-d7e9-8f75c97eeba7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('state_union')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/state_union.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-VE_m_I6Jp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6-JfsyWI-v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tokenized = custom_sent_tokenizer.tokenize(train_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLA0YV96qq4A",
        "colab_type": "code",
        "outputId": "d0f93beb-d699-40b8-b542-eec638dcd3e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install dateparser"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/30/5cb8bb214c0b111fb59137c2e19c636a136209dbe45e1c3e9d63f7a76c1a/dateparser-0.7.1-py2.py3-none-any.whl (351kB)\n",
            "\r\u001b[K     |█                               | 10kB 13.5MB/s eta 0:00:01\r\u001b[K     |█▉                              | 20kB 3.2MB/s eta 0:00:01\r\u001b[K     |██▉                             | 30kB 4.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 81kB 5.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 102kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 112kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 122kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 133kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 143kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 153kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 163kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 174kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 184kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 194kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 204kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 215kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 225kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 235kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 245kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 256kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 266kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 276kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 286kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 296kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 307kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 317kB 5.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 327kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 337kB 5.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 348kB 5.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 358kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser) (2.5.3)\n",
            "Collecting regex (from dateparser)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 52.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser) (1.12.0)\n",
            "Building wheels for collected packages: regex\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609242 sha256=8f28fc2c162ae05e5456d48ad0ef9140e2b15834c8ea79f5d06773b8626d37ed\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "Successfully built regex\n",
            "Installing collected packages: regex, dateparser\n",
            "Successfully installed dateparser-0.7.1 regex-2019.8.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vY7_i2uqykr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dateutil.parser import parse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlLHUwv15act",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e56wq-ti5dn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dateutil.parser as dparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZotnUeYNpod3",
        "colab_type": "code",
        "outputId": "d3aba4a4-74fe-41a6-9f26-295bb596985a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dparser.parse(\"monkey 07-10 love banana\",fuzzy=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2019, 7, 10, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1KTxBpJpvRY",
        "colab_type": "code",
        "outputId": "7fa2822f-8c94-4ee6-9eb7-d004d466d68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dparser.parse(\"monkey 20/01/1980 love banana\",fuzzy=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(1980, 1, 20, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3BbQuzap_gh",
        "colab_type": "code",
        "outputId": "e995aefb-db4a-46b1-f37e-62944a754ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dparser.parse(\"monkey 20/01/1980 love banana and 2018-07-10 \",fuzzy=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(1980, 1, 20, 20, 18, tzinfo=tzoffset(None, -36000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaFbiMQisv93",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datefinder\n",
        "stringgg=  \"\"\"\n",
        "    2305 TO THE PROFESSIONAL SERVICES GLOBAL SUPPLY AGREEMENT DATED 23-12-2013 (   SoW    or    SOW   ) BETWEEN ABCX Software Development (India) Private Limited, Pune 411 006, India, and place of business at Yerwada, Pune 411006, India, and hereinafter referred to as    ABCX    or    ABCX Contracting Party   ;  and  Infosys Limited whose registered office is at a corporation organized and existing under the laws of India and having its primary place of business at Electronics City, Hosur Road, Bangalore 560 100, India (Registered Number India: L85110KA1981PLC013115) and hereinafter referred to as    Supplier    or    Infosys     (Together, for the purposes of this SoW, the    Parties   ) IT IS AGREED THAT:   The Terms and Conditions of the Professional Services Global Supply Agreement dated 25 November 2013 between ABCX Holdings plc and Infosys Limited, Bangalore, India and all Attachments, amendments and variations made thereto in accordance with the terms thereof (the    Global Terms and Conditions   ) are incorporated into this SoW as if written out in full herein.\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqKYP9IKosfu",
        "colab_type": "code",
        "outputId": "0b705df6-87f5-4acd-85f0-6cdd4cdce726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8Kah2n1ovHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjbligyio2C8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentences = \"2305  TO THE PROFESSIONAL SERVICES GLOBAL SUPPLY AGREEMENT DATED  2013 January 11 (   SoW    or    SOW   ) BETWEEN ABCX Software Development (India) Private Limited, Pune 411 006, India, and place of business at Yerwada, Pune 411006, India, and hereinafter referred to as    ABCX    or    ABCX Contracting Party   ;  and  Infosys Limited whose registered office is at a corporation organized and existing under the laws of India and having its primary place of business at Electronics City, Hosur Road, Bangalore 560 100, India (Registered Number India: L85110KA1981PLC013115) and hereinafter referred to as    Supplier    or    Infosys     (Together, for the purposes of this SoW, the    Parties   ) IT IS AGREED THAT:   The Terms and Conditions of the Professional Services Global Supply 2016-12-22 Agreement dated 25 November 2013 between ABCX Holdings plc and Infosys Limited, Bangalore, India and all Attachments, amendments and variations made thereto in accordance with the terms thereof (the    Global Terms and Conditions   ) 2013-11 are incorporated into this SoW as if written out in full herein.\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrRzbNWUHZY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences= \"2003 entries are due by January 4 2017 at 8:00pm created 2016-12-22 as discussed in 20/11/2007\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xt8pFlgpH_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = nltk.word_tokenize(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJBY-f9r1rBW",
        "colab_type": "code",
        "outputId": "91662876-e0ea-4c37-dd84-41748b179d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# word[1]\n",
        "len(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "404Pie05HN1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dateutil.parser import parse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KF6i7Q6tuoN0",
        "colab_type": "code",
        "outputId": "88c538af-e4fe-43c2-e03a-1bf1e157a22e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#word\n",
        "date_list = []\n",
        "for i in range(len(word)):\n",
        "  words = word[i]\n",
        "  months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]\n",
        "  matches = datefinder.find_dates(words[i-2:i+2])\n",
        "  \n",
        "  \n",
        "  if(len(words)==4 and words.isnumeric()):\n",
        "      \n",
        "\n",
        "      if (word[i-2] in months):\n",
        "          \n",
        "          date_list.append(parse(words+\" \"+word[i-2]))\n",
        "          for match in matches:\n",
        "            print(match)\n",
        "         \n",
        "      if (word[i-1] in months): \n",
        "\n",
        "\n",
        "          date_list.append(parse(words+\" \"+word[i-1]))\n",
        "#           for match in matches:\n",
        "#             print(match)\n",
        "      if (word[i+1] in months):\n",
        "\n",
        "           date_list.append(parse(words+\" \"+word[i+1]))\n",
        "#            for match in matches:\n",
        "#               print(match)\n",
        "        \n",
        "  if (len(words)>4 ):\n",
        "    \n",
        "     \n",
        "#     print(words)\n",
        "    mat = re.match('(\\d{4})(/|-|.)(January|February|March|April|May|June|July|August|September|October|November|December|(\\d{2}))(/|-|.)(\\d{2})', words)\n",
        "    matt = re.match('(January|February|March|April|May|June|July|August|September|October|November|December|(\\d{2}))(/|-|.)(\\d{2})(/|-|.)(\\d{4})', words)\n",
        "\n",
        "    if mat is not None:\n",
        "        date_list.append(parse(words))\n",
        "#         for match in matches:\n",
        "#           print(match)\n",
        "    if matt is not None:\n",
        "        date_list.append(parse(words))  \n",
        "#     if mattt is not None:\n",
        "#         date_list.append(parse(words))\n",
        "\n",
        "date_list\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[datetime.datetime(2017, 1, 5, 0, 0),\n",
              " datetime.datetime(2016, 12, 22, 0, 0),\n",
              " datetime.datetime(2007, 11, 20, 0, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Nd897mHXtw",
        "colab_type": "code",
        "outputId": "550569a0-2777-4917-8dab-1ea0aeb127d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sentences= \"\"\"2003 entries are due by January 4 2017 at 8:00pm created 2016-12-22 as discussed in 12 11 2007 ok hello ok \"\"\"\n",
        "word = nltk.word_tokenize(sentences)\n",
        "#word\n",
        "date_list = []\n",
        "for i in range(len(word)):\n",
        "  words = word[i]\n",
        "  months = [\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\",\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",\"10\",\"11\",\"12\"]\n",
        "  \n",
        "  \n",
        "  \n",
        "  if(len(words)==4 and words.isnumeric()):\n",
        "      \n",
        "      print(words)\n",
        "      if (word[i-2] in months):\n",
        "#         print(i)\n",
        "        a = \" \"\n",
        "        b = 0\n",
        "        c = 0\n",
        "        b = i-2\n",
        "        c= i + 2\n",
        "#         print(b,\"ok\",c)\n",
        "        word[b:c]\n",
        "        for k in range(b,c):\n",
        "#           print(k)\n",
        "          a = a + \" \"+word[k]\n",
        "#         print(a)\n",
        "        matches = datefinder.find_dates(a)\n",
        "        \n",
        "        date_list.append(parse(words+\" \"+word[i-2]))\n",
        "        for match in matches:\n",
        "          print(match)\n",
        "         \n",
        "#       if (word[i-1] in months): \n",
        "\n",
        "\n",
        "#           date_list.append(parse(words+\" \"+word[i-1]))\n",
        "# #           for match in matches:\n",
        "# #             print(match)\n",
        "#       if (word[i+1] in months):\n",
        "\n",
        "#            date_list.append(parse(words+\" \"+word[i+1]))\n",
        "# #            for match in matches:\n",
        "# #               print(match)\n",
        "        \n",
        "#   if (len(words)>4 ):\n",
        "    \n",
        "     \n",
        "# #     print(words)\n",
        "#     mat = re.match('(\\d{4})(/|-|.)(January|February|March|April|May|June|July|August|September|October|November|December|(\\d{2}))(/|-|.)(\\d{2})', words)\n",
        "#     matt = re.match('(January|February|March|April|May|June|July|August|September|October|November|December|(\\d{2}))(/|-|.)(\\d{2})(/|-|.)(\\d{4})', words)\n",
        "\n",
        "#     if mat is not None:\n",
        "#         date_list.append(parse(words))\n",
        "# #         for match in matches:\n",
        "# #           print(match)\n",
        "#     if matt is not None:\n",
        "#         date_list.append(parse(words))  \n",
        "# #     if mattt is not None:\n",
        "# #         date_list.append(parse(words))\n",
        "\n",
        "# date_list\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2003\n",
            "2017\n",
            "2017-01-04 00:00:00\n",
            "2007\n",
            "2007-12-11 00:00:00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}